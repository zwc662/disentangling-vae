{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO)\n",
    "---\n",
    "In this notebook, we train PPO with plain pixel-wise perturbation environment.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.math'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fcb639b37283>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlp_policy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlp_critic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mValue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mppo_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/export/u1/homes/weichao/Workspace/disentangling-vae/exp_adv/PPO/mlp_policy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.math'"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import dynamics\n",
    "importlib.reload(dynamics)\n",
    "from dynamics import Dynamics\n",
    "\n",
    "import sys\n",
    "sys.path.append('./PPO/')\n",
    "\n",
    "from utils import *\n",
    "from mlp_policy import Policy\n",
    "from mlp_critic import Value\n",
    "from core.ppo import ppo_step\n",
    "from core.common import estimate_advantages\n",
    "from core.agent import Agent\n",
    "\n",
    "dtype = torch.float64\n",
    "torch.set_default_dtype(dtype)\n",
    "device = torch.device('cuda', index = args.gpu_index) if torch.cuda.is_available() else torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(args.gpu_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Dynamics(dataset = 'mnist', vae = 'VAE_mnist', cls = 'CLS_mnist', target = 9)\n",
    "env.reset()\n",
    "state_size = env.state_size()\n",
    "action_size = env.action_size()\n",
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -948.35\n",
      "Episode 200\tAverage Score: -948.68\n",
      "Episode 300\tAverage Score: -948.68\n",
      "Episode 400\tAverage Score: -948.68\n",
      "Episode 500\tAverage Score: -948.68\n",
      "Episode 600\tAverage Score: -948.68\n",
      "Episode 700\tAverage Score: -948.68\n",
      "Episode 800\tAverage Score: -948.68\n",
      "Episode 900\tAverage Score: -948.68\n",
      "Episode 1000\tAverage Score: -948.68\n",
      "Episode 1100\tAverage Score: -948.68\n",
      "Episode 1200\tAverage Score: -948.68\n",
      "Episode 1300\tAverage Score: -948.68\n",
      "Episode 1400\tAverage Score: -948.68\n",
      "Episode 1500\tAverage Score: -948.68\n",
      "Episode 1600\tAverage Score: -948.68\n",
      "Episode 1700\tAverage Score: -948.68\n",
      "Episode 1800\tAverage Score: -948.68\n",
      "Episode 1900\tAverage Score: -948.68\n",
      "Episode 2000\tAverage Score: -948.68\n",
      "Episode 2100\tAverage Score: -948.68\n",
      "Episode 2200\tAverage Score: -948.68\n",
      "Episode 2300\tAverage Score: -948.68\n",
      "Episode 2362\tAverage Score: -948.68"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1128d9bb2833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-1128d9bb2833>\u001b[0m in \u001b[0;36mddpg\u001b[0;34m(n_episodes, max_t, print_every)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/export/u1/homes/weichao/Workspace/disentangling-vae/exp_adv/ddpg_agent.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, add_noise)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/disentanglement_challenge/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/export/u1/homes/weichao/Workspace/disentangling-vae/exp_adv/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/disentanglement_challenge/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/disentanglement_challenge/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/disentanglement_challenge/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 340\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def ddpg(n_episodes=10000, max_t=300, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "    return scores\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch a Smart Agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-20.030639093362726 False\n",
      "-3872.7560537558 False\n",
      "-3814.1891055153455 False\n",
      "-3847.071071732393 False\n",
      "-3847.235048856188 False\n",
      "-3883.7205412791177 False\n",
      "-3856.5942801737847 False\n",
      "-3938.4364384628357 False\n",
      "-3848.2640285111243 False\n",
      "-3864.712574613516 False\n",
      "-3890.0591526860585 False\n",
      "-3887.154923866444 False\n",
      "-3815.3349221751105 False\n",
      "-3919.779576944705 False\n",
      "-3814.6427619502856 False\n",
      "-3899.882242669465 False\n",
      "-3849.6326178992063 False\n",
      "-3915.5297254360485 False\n",
      "-3832.3922267187795 False\n",
      "-3883.1383394974214 False\n",
      "-3841.4541112790334 False\n",
      "-3871.5215972598353 False\n",
      "-3793.070107809585 False\n",
      "-3894.8703980375253 False\n",
      "-3817.175592979543 False\n",
      "-3921.784381949799 False\n",
      "-3886.4961287202254 False\n",
      "-3865.5335189619336 False\n",
      "-3872.723037845125 False\n",
      "-3879.5586626183763 False\n",
      "-3859.685907582471 False\n",
      "-3868.208791225044 False\n",
      "-3922.323519392859 False\n",
      "-3814.273924015916 False\n",
      "-3864.169002066616 False\n",
      "-3867.0410888726265 False\n",
      "-3859.4003732343144 False\n",
      "-3940.2572250151243 False\n",
      "-3846.5198443218883 False\n",
      "-3858.6424760900436 False\n",
      "-3891.8903059442964 False\n",
      "-3849.621092376081 False\n",
      "-3886.2130186362856 False\n",
      "-3866.571047887233 False\n",
      "-3907.8917824150476 False\n",
      "-3813.143639354407 False\n",
      "-3938.6233091072277 False\n",
      "-3916.5260998957624 False\n",
      "-3882.7934019663576 False\n",
      "-3881.6018121798247 False\n",
      "-3874.79032900615 False\n",
      "-3860.834460860876 False\n",
      "-3878.3227330495524 False\n",
      "-3874.0994044921135 False\n",
      "-3855.2425002421814 False\n",
      "-3818.0005226554745 False\n",
      "-3903.6516634440427 False\n",
      "-3870.8517455424285 False\n",
      "-3855.799846792248 False\n",
      "-3940.439424743503 False\n",
      "-3889.685976813206 False\n",
      "-3846.6929590155582 False\n",
      "-3840.4810623165063 False\n",
      "-3855.277144035694 False\n",
      "-3906.8113471525726 False\n",
      "-3897.4823069267777 False\n",
      "-3896.5871198592013 False\n",
      "-3848.40094015586 False\n",
      "-3905.3527696568435 False\n",
      "-3832.483970464078 False\n",
      "-3881.4425017066164 False\n",
      "-3862.834155910643 False\n",
      "-3871.6701099903726 False\n",
      "-3816.059835343293 False\n",
      "-3890.5927943737915 False\n",
      "-3840.730597931931 False\n",
      "-3930.8456320124387 False\n",
      "-3864.0606787496354 False\n",
      "-3840.6668181430564 False\n",
      "-3865.367754272982 False\n",
      "-3858.4344971603664 False\n",
      "-3883.498243329735 False\n",
      "-3856.9245938878207 False\n",
      "-3915.276893657457 False\n",
      "-3858.759508674357 False\n",
      "-3890.212531697164 False\n",
      "-3856.3394770024274 False\n",
      "-3897.5285712124883 False\n",
      "-3827.9123413164502 False\n",
      "-3849.8092106067816 False\n",
      "-3811.8016386156323 False\n",
      "-3823.1775472586182 False\n",
      "-3853.6691179582112 False\n",
      "-3914.7324710721336 False\n",
      "-3871.2599602311184 False\n",
      "-3863.2352549524753 False\n",
      "-3899.1063250191005 False\n",
      "-3815.8019858179955 False\n",
      "-3920.252922580611 False\n",
      "-3873.3506680625555 False\n",
      "-3907.428866931442 False\n",
      "-3847.917979352616 False\n",
      "-3862.1711874342354 False\n",
      "-3884.291285211582 False\n",
      "-3897.0033849327547 False\n",
      "-3820.616549502002 False\n",
      "-3850.387113212567 False\n",
      "-3917.614341674256 False\n",
      "-3907.24405507536 False\n",
      "-3859.004203351035 False\n",
      "-3905.5766164133684 False\n",
      "-3879.4310191850786 False\n",
      "-3838.937629164401 False\n",
      "-3916.40187386802 False\n",
      "-3863.7169013989237 False\n",
      "-3832.1569320765766 False\n",
      "-3871.3583861157845 False\n",
      "-3896.974029986519 False\n",
      "-3850.3343568061164 False\n",
      "-3896.955669514006 False\n",
      "-3899.0194995889783 False\n",
      "-3845.8881280912256 False\n",
      "-3862.7337087573164 False\n",
      "-3874.803947872486 False\n",
      "-3924.9465079174734 False\n",
      "-3904.776645560109 False\n",
      "-3874.422450093224 False\n",
      "-3855.7963182202584 False\n",
      "-3923.835455864891 False\n",
      "-3882.125769304202 False\n",
      "-3883.197564817903 False\n",
      "-3854.7236107187296 False\n",
      "-3897.2884999191633 False\n",
      "-3832.587205197929 False\n",
      "-3881.794801740829 False\n",
      "-3815.0962280989743 False\n",
      "-3905.9339187952714 False\n",
      "-3851.1349093815465 False\n",
      "-3923.6211975291267 False\n",
      "-3837.0244465197356 False\n",
      "-3897.082118986764 False\n",
      "-3871.4935616007047 False\n",
      "-3855.2893099812536 False\n",
      "-3874.670098543528 False\n",
      "-3862.143922404173 False\n",
      "-3888.23618011422 False\n",
      "-3865.500273761925 False\n",
      "-3858.260481865905 False\n",
      "-3858.619098530575 False\n",
      "-3895.850642301343 False\n",
      "-3858.4761444833734 False\n",
      "-3892.476192794524 False\n",
      "-3840.342044295696 False\n",
      "-3814.4535648406745 False\n",
      "-3852.3179900255113 False\n",
      "-3848.2880977901227 False\n",
      "-3907.6727564699922 False\n",
      "-3879.552047840366 False\n",
      "-3897.5481875738637 False\n",
      "-3816.4772846989868 False\n",
      "-3900.2659046869226 False\n",
      "-3858.864564063404 False\n",
      "-3879.0891135845104 False\n",
      "-3892.9201976017353 False\n",
      "-3867.538101869447 False\n",
      "-3865.2433567578687 False\n",
      "-3875.5599090402575 False\n",
      "-3897.161194865715 False\n",
      "-3849.872458399499 False\n",
      "-3906.7438915986477 False\n",
      "-3881.9043405233656 False\n",
      "-3866.417788822455 False\n",
      "-3841.884284553392 False\n",
      "-3916.029587332746 False\n",
      "-3843.5196137573694 False\n",
      "-3874.264970414656 False\n",
      "-3882.6223894587947 False\n",
      "-3863.9500208942145 False\n",
      "-3900.05089593631 False\n",
      "-3823.0838626609707 False\n",
      "-3862.9017428087545 False\n",
      "-3890.522835474536 False\n",
      "-3870.6039402694705 False\n",
      "-3815.4441247424115 False\n",
      "-3920.866094288274 False\n",
      "-3823.45047400021 False\n",
      "-3854.5437034514875 False\n",
      "-3817.182071811172 False\n",
      "-3905.582804714147 False\n",
      "-3873.2755123839947 False\n",
      "-3874.0336290075134 False\n",
      "-3876.260651060534 False\n",
      "-3875.607437943502 False\n",
      "-3908.3946689291347 False\n",
      "-3840.9911903248512 False\n",
      "-3915.3148111170235 False\n",
      "-3823.7228694326327 False\n",
      "-3898.6510007689776 False\n",
      "-3858.0130077522626 False\n",
      "-3842.395262346131 False\n"
     ]
    }
   ],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "\n",
    "state = env.reset()\n",
    "for t in range(200):\n",
    "    action = agent.act(state, add_noise=False)\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    print(reward, done)\n",
    "    if done:\n",
    "        break \n",
    "img = env.render()\n",
    "img.show()\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Explore\n",
    "\n",
    "In this exercise, we have provided a sample DDPG agent and demonstrated how to use it to solve an OpenAI Gym environment.  To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n",
    "- Amend the various hyperparameters and network architecture to see if you can get your agent to solve the environment faster than this benchmark implementation.  Once you build intuition for the hyperparameters that work well with this environment, try solving a different OpenAI Gym task!\n",
    "- Write your own DDPG implementation.  Use this code as reference only when needed -- try as much as you can to write your own algorithm from scratch.\n",
    "- You may also like to implement prioritized experience replay, to see if it speeds learning.  \n",
    "- The current implementation adds Ornsetein-Uhlenbeck noise to the action space.  However, it has [been shown](https://blog.openai.com/better-exploration-with-parameter-noise/) that adding noise to the parameters of the neural network policy can improve performance.  Make this change to the code, to verify it for yourself!\n",
    "- Write a blog post explaining the intuition behind the DDPG algorithm and demonstrating how to use it to solve an RL environment of your choosing.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
